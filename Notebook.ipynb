{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "49e61424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('input.txt', <http.client.HTTPMessage at 0x1f03f80cb10>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\", \"input.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "35f537ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text=f.read()\n",
    "print(f'Length of text: {len(text)} characters')\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "75cfb86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "chars=sorted(list(set(text)))\n",
    "vocab_size=len(chars)\n",
    "print(''.join(chars)) \n",
    "print(f'Vocab size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e369ea9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42]\n",
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "# tokenizer\n",
    "s_to_i = { ch:i for i,ch in enumerate(chars) }\n",
    "i_to_s = { i:ch for i,ch in enumerate(chars) }\n",
    "encode=lambda s: [s_to_i[c] for c in s]\n",
    "decode=lambda l: ''.join([i_to_s[i] for i in l])\n",
    "\n",
    "print(encode(\"Hello World\"))\n",
    "print(decode(encode(\"Hello World\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "10d4af79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# encoding dateser and storing in Tensor\n",
    "\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])  # first 1000 characters encoded as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "47f6f74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train validation split\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "de2cfaf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length=8\n",
    "train_data[:context_length+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "39f5f619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[63,  1, 57, 41, 43, 54, 58, 56],\n",
      "        [58,  6,  1, 42, 43, 39, 56,  1],\n",
      "        [ 1, 46, 47, 51,  1, 58, 46, 39],\n",
      "        [42, 59, 58, 63,  0, 32, 53,  1]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 1, 57, 41, 43, 54, 58, 56, 43],\n",
      "        [ 6,  1, 42, 43, 39, 56,  1, 51],\n",
      "        [46, 47, 51,  1, 58, 46, 39, 58],\n",
      "        [59, 58, 63,  0, 32, 53,  1, 44]])\n",
      "-----------------\n",
      "when input is [63] the target: 1\n",
      "when input is [63, 1] the target: 57\n",
      "when input is [63, 1, 57] the target: 41\n",
      "when input is [63, 1, 57, 41] the target: 43\n",
      "when input is [63, 1, 57, 41, 43] the target: 54\n",
      "when input is [63, 1, 57, 41, 43, 54] the target: 58\n",
      "when input is [63, 1, 57, 41, 43, 54, 58] the target: 56\n",
      "when input is [63, 1, 57, 41, 43, 54, 58, 56] the target: 43\n",
      "when input is [58] the target: 6\n",
      "when input is [58, 6] the target: 1\n",
      "when input is [58, 6, 1] the target: 42\n",
      "when input is [58, 6, 1, 42] the target: 43\n",
      "when input is [58, 6, 1, 42, 43] the target: 39\n",
      "when input is [58, 6, 1, 42, 43, 39] the target: 56\n",
      "when input is [58, 6, 1, 42, 43, 39, 56] the target: 1\n",
      "when input is [58, 6, 1, 42, 43, 39, 56, 1] the target: 51\n",
      "when input is [1] the target: 46\n",
      "when input is [1, 46] the target: 47\n",
      "when input is [1, 46, 47] the target: 51\n",
      "when input is [1, 46, 47, 51] the target: 1\n",
      "when input is [1, 46, 47, 51, 1] the target: 58\n",
      "when input is [1, 46, 47, 51, 1, 58] the target: 46\n",
      "when input is [1, 46, 47, 51, 1, 58, 46] the target: 39\n",
      "when input is [1, 46, 47, 51, 1, 58, 46, 39] the target: 58\n",
      "when input is [42] the target: 59\n",
      "when input is [42, 59] the target: 58\n",
      "when input is [42, 59, 58] the target: 63\n",
      "when input is [42, 59, 58, 63] the target: 0\n",
      "when input is [42, 59, 58, 63, 0] the target: 32\n",
      "when input is [42, 59, 58, 63, 0, 32] the target: 53\n",
      "when input is [42, 59, 58, 63, 0, 32, 53] the target: 1\n",
      "when input is [42, 59, 58, 63, 0, 32, 53, 1] the target: 44\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(18)\n",
    "batch_size=4\n",
    "block_size=8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "\n",
    "\n",
    "print(\"-----------------\")\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'when input is {context.tolist()} the target: {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6a890cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.9172, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "E oRvxfSFnJS,XrkdQbZQaZ\n",
      "dkIvvwsSZxiLaKQoQDAyaC&$ubNLQiYaN.I?s.juJ,Z-kE:u,czIAR?gbNMweNUydsEjBj'Kqb?i\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(18)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)   \n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C=logits.shape\n",
    "            logits=logits.view(B*T,C)\n",
    "            targets=targets.view(B*T) \n",
    "            loss=F.cross_entropy(logits,targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :] \n",
    "\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            idx_next = torch.multinomial(probs, num_samples=1) \n",
    "            idx = torch.cat((idx, idx_next), dim=1) \n",
    "        return idx\n",
    "\n",
    "m=BigramLanguageModel(vocab_size)\n",
    "logits,loss=m(xb,yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "idx=torch.zeros((1,1),dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "728783ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f9019a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 2.491960048675537\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "\n",
    "for steps in range(10000):\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"Final loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "003b4322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F t a ie duseeray w\n",
      "promavevedrt! t$UE rhalodforLI medbe, oucoco ele t hl ameathandyourad, ggatha,\n",
      "I cowot my\n",
      "\n",
      "\n",
      "Hiron; bay oun er, thaids ath--mes therreshaucon3DOfal\n",
      "The y t m byoimof th;\n",
      "TEThou w!\n",
      "DYofelo ksnge.\n",
      "Bild we:\n",
      "INour&&Fimyo flly, pr mabupel; inof,\n",
      "Woth nsthes cetenese, d hol mares t mbe sengr loitoth deve;\n",
      "Ast; a seane\n",
      "O:\n",
      "RCI INCENGouthenenk's t mufe his ind, as, k-\n",
      "ge ththur cknoul waded, s agef io d airoouthinga; it al hond r, so, wid equ chaloy.\n",
      "Tha, hand th m O tispeair:\n",
      "\n",
      "TEXDo brese for ccaverevebl HAlou pond r be.\n",
      "BHOlit, d e, viu d s ishy haned\n",
      "Dile J.\n",
      "G th's,\n",
      "\n",
      "\n",
      "es\n",
      "The mphy dveda thous!\n",
      "t dofanl.\n",
      "cerise s f m,\n",
      "\n",
      "Gerail lldidkis w; fle\n",
      "Ficturs bs ceqQ'd k wind ourequlaltal hyodee, th\n",
      "oothr TCanQ'sed, e wale tth.\n",
      "OUCULimllocshawenondels louroknviow as owarir s fo wrivomas\n",
      "llout, WAPo f? play qur, lepe.\n",
      "\n",
      "O:\n",
      "KI\n",
      "I it---t ak wiD sel t stistin choo w; wf mant inel k, wioranorgr f w notik; mouthy by noue y.\n",
      "\n",
      "Thoureretl that.\n",
      "\n",
      "R:\n",
      "JWAhou? ancorthenchons mard;\n",
      "\n",
      "Pamor wiss f,oula\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "57b9589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(100)\n",
    "        for k in range(100):\n",
    "            xb, yb = get_batch(split)\n",
    "            logits, loss = m(xb, yb)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8146c6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "B,T,C=4,8,2\n",
    "x=torch.randn((B,T,C))\n",
    "\n",
    "#v1\n",
    "xbow=torch.zeros(B,T,C)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]\n",
    "        xbow[b,t]=torch.mean(xprev,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f75610a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#v2\n",
    "wei=torch.tril(torch.ones(T,T))\n",
    "wei=wei/wei.sum(1,keepdim=True)\n",
    "\n",
    "xbow2=wei@x\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cda548d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#v3\n",
    "tril=torch.tril(torch.ones(T,T))\n",
    "wei=torch.zeros((T,T))\n",
    "wei=wei.masked_fill(tril==0, float('-inf'))\n",
    "wei=F.softmax(wei,dim=-1)\n",
    "xbow3=wei@x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfdb0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #v4\n",
    "\n",
    "torch.manual_seed(18)\n",
    "B,T,C=4,8,32\n",
    "x=torch.randn(B,T,C)\n",
    "\n",
    "head_size=16\n",
    "\n",
    "key=nn.Linear(C, head_size, bias=False)\n",
    "query=nn.Linear(C, head_size, bias=False)\n",
    "value=nn.Linear(C, head_size, bias=False)\n",
    "k=key(x)   # (B,T,head_size)\n",
    "q=query(x) # (B,T,head_size)\n",
    "v=value(x)\n",
    "wei=q@k.transpose(-2,-1)*head_size**-0.5\n",
    "\n",
    "\n",
    "tril=torch.tril(torch.ones(T,T))\n",
    "wei=torch.zeros((T,T))\n",
    "wei=wei.masked_fill(tril==0, float('-inf'))\n",
    "wei=F.softmax(wei,dim=-1)\n",
    "out=wei@v\n",
    "out.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c80b5b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed=32\n",
    "block_size=128\n",
    "batch_size=32\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key=nn.Linear(C, head_size, bias=False)\n",
    "        self.query=nn.Linear(C, head_size, bias=False)\n",
    "        self.value=nn.Linear(C, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout=nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C=x.shape\n",
    "        k=self.key(x)   # (B,T,head_size)\n",
    "        q=self.query(x) # (B,T,head_size)\n",
    "\n",
    "        wei=q@k.transpose(-2,-1)*head_size**-0.5\n",
    "        wei=wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
    "        wei=F.softmax(wei,dim=-1)\n",
    "        wei=self.dropout(wei)\n",
    "        v=self.value(x)\n",
    "        out=wei@v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads=nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.projection=nn.Linear(n_embed,n_embed)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out=torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out=self.projection(out)\n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(\n",
    "            nn.Linear(n_embed,4*n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embed, n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa=MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd=FeedForward(n_embed)\n",
    "        self.ln1=nn.LayerNorm(n_embed)\n",
    "        self.ln2=nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x =  x + self.sa(self.ln1(x))\n",
    "        x =  x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2c14d9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss 4.441918849945068\n",
      "step 100: loss 3.056455135345459\n",
      "step 200: loss 2.709581136703491\n",
      "step 300: loss 2.577690839767456\n",
      "step 400: loss 2.5531868934631348\n",
      "step 500: loss 2.4875001907348633\n",
      "step 600: loss 2.4918079376220703\n",
      "step 700: loss 2.4628536701202393\n",
      "step 800: loss 2.3984739780426025\n",
      "step 900: loss 2.451901435852051\n",
      "step 1000: loss 2.4115681648254395\n",
      "step 1100: loss 2.3755342960357666\n",
      "step 1200: loss 2.368553638458252\n",
      "step 1300: loss 2.3368382453918457\n",
      "step 1400: loss 2.3147060871124268\n",
      "step 1500: loss 2.3360044956207275\n",
      "step 1600: loss 2.304831027984619\n",
      "step 1700: loss 2.2697925567626953\n",
      "step 1800: loss 2.2710354328155518\n",
      "step 1900: loss 2.2163712978363037\n",
      "step 2000: loss 2.1960935592651367\n",
      "step 2100: loss 2.2169835567474365\n",
      "step 2200: loss 2.1868064403533936\n",
      "step 2300: loss 2.212981700897217\n",
      "step 2400: loss 2.152888536453247\n",
      "step 2500: loss 2.1642351150512695\n",
      "step 2600: loss 2.112882137298584\n",
      "step 2700: loss 2.1534698009490967\n",
      "step 2800: loss 2.1257407665252686\n",
      "step 2900: loss 2.1198675632476807\n",
      "step 3000: loss 2.088015556335449\n",
      "step 3100: loss 2.1281678676605225\n",
      "step 3200: loss 2.1125245094299316\n",
      "step 3300: loss 2.0631418228149414\n",
      "step 3400: loss 2.075890302658081\n",
      "step 3500: loss 2.0233876705169678\n",
      "step 3600: loss 2.014492988586426\n",
      "step 3700: loss 2.0060324668884277\n",
      "step 3800: loss 2.0481512546539307\n",
      "step 3900: loss 2.029085397720337\n",
      "step 4000: loss 2.0422661304473877\n",
      "step 4100: loss 2.017723798751831\n",
      "step 4200: loss 2.011066436767578\n",
      "step 4300: loss 2.06208872795105\n",
      "step 4400: loss 1.9884272813796997\n",
      "step 4500: loss 1.954837441444397\n",
      "step 4600: loss 1.9529085159301758\n",
      "step 4700: loss 2.020399332046509\n",
      "step 4800: loss 1.9800395965576172\n",
      "step 4900: loss 1.9850523471832275\n",
      "Final loss: 1.9828451871871948\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(18)\n",
    "\n",
    "n_embed=32\n",
    "B,T,C=4,8,32\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)   \n",
    "        self.position_embedding_table=nn.Embedding(block_size,n_embed)\n",
    "        self.lm_head=nn.Linear(n_embed, vocab_size)\n",
    "        self.ffwd=FeedForward(n_embed)\n",
    "        self.sa_heads=MultiHeadAttention(num_heads=4, head_size=n_embed//4)\n",
    "        self.blocks=nn.Sequential(\n",
    "            Block(n_embed, n_head=4),\n",
    "            Block(n_embed, n_head=4),\n",
    "            Block(n_embed, n_head=4),\n",
    "            nn.LayerNorm(n_embed),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        token_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb=self.position_embedding_table(torch.arange(T))\n",
    "        x=token_emb+pos_emb\n",
    "        x=self.blocks(x)\n",
    "        logits = self.lm_head(x)  # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C=logits.shape\n",
    "            logits_flat=logits.view(B*T,C)\n",
    "            targets_flat=targets.view(B*T) \n",
    "            loss=F.cross_entropy(logits_flat,targets_flat)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond=idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :] \n",
    "\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            idx_next = torch.multinomial(probs, num_samples=1) \n",
    "            idx = torch.cat((idx, idx_next), dim=1) \n",
    "        return idx\n",
    "\n",
    "m=LanguageModel()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "\n",
    "idx=torch.zeros((1,1),dtype=torch.long)\n",
    "\n",
    "for steps in range(5000):\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % 100 == 0:\n",
    "        print(f\"step {steps}: loss {loss.item()}\")\n",
    "\n",
    "print(f\"Final loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0a5eca0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ORCKINTI:\n",
      "By frow, I have shen thesbuned of Mingue.\n",
      "\n",
      "LUCIO:\n",
      "Dumede, I minade cin thy dist\n",
      "All kict and a ence thal naw'dsed, But cancent,\n",
      "I'l to langucht jace, not that on garen to heas\n",
      "Pad deevf sat condser to of shall swikel prom Moosss\n",
      "Un toued hase sut lomonce I byou think me sicblon\n",
      "To his shing tus this pred batht ur theat you?\n",
      "Neor congsmilict Bears is your have knop Farde!\n",
      "\n",
      "MELABENES:\n",
      "Goured handse ding, poth now wich whas!\n",
      "DWit Hest nothan, nif and I swan no the.\n",
      "\n",
      "KINE VINCE ENLOM:\n",
      "Whreard alsourd ling'ds wos you ching's,\n",
      "This-for come weordn hokese crioos.\n",
      "Pret this be nour nour I'lonem:\n",
      "He all hand not pight to is rince yewf\n",
      "Witillor, uth mee of that tworeftin.\n",
      "\n",
      "HESASS:\n",
      "O, bardie yould le merve copages eyou,\n",
      "What, of And thims grath with as gooor you.\n",
      "ISABR AUCENT:\n",
      "Younk hore own make for with compp\n",
      "A werence nin and to say ther onelficr:\n",
      "Whose that relforke profionond.\n",
      "\n",
      "For prepntitle heas bone bow gron that this th\n",
      "That ifonqua utes shery ous nie nive,\n",
      "Thix fe deckell nages the ablt yous lowss!\n",
      "Gind mertr, cofor t pits: apit you tho seelll\n",
      "ore e hances guse and upse hor thous and withe feirs\n",
      "Teeir to st.\n",
      "\n",
      "\n",
      "KINGHARGHARD:\n",
      "Sorme not yound of frentors?\n",
      "\n",
      "QUSAENETE:\n",
      "I Grightel wering Bing; staken a gnindy\n",
      "And the des theror.\n",
      "\n",
      "DULIABENGTARES:\n",
      "I ham, he wreeedmin. Buldst Soinn but anots,\n",
      "Incat lig shan? be thower pace my his not.\n",
      "MARe, to to her ut decch, that dandss bact.\n",
      "Witherss well-fprow ann no burer for Cand\n",
      "Thou ofres, soolvice; in ra lave makes,\n",
      "We lech thor,-\n",
      "Bechare's of refin, grone and chinjut of it blove tre\n",
      "Chey honou to the your ared is Sit mowet\n",
      "itrling qutck it.\n",
      "\n",
      "LUCHASTING:\n",
      "You Beh e ings, And:\n",
      "Sa is neworld my; is tisll-it thou here will foe?\n",
      "D ABELIl willst blare: digedo, rices know;\n",
      "Frixtl's:\n",
      "U? Gout.\n",
      "Shad in t peence ace umpak pontlo,\n",
      "And wharges thou, hese, it o fridedlat,\n",
      "Artid a thy perve nat my 'Ubuserws that ppoont:\n",
      "Not by farters and naiser frearsustred hems now\n",
      "Your you?\n",
      "\n",
      "\n",
      "MARWESMINE:\n",
      "Set hat 'e gres; it.\n",
      "Share me here wan leared tis theer to is ware as hobope\n",
      "Ter a youre in but of alll a\n",
      "Monte terteres ofor beve it spvons, nand thing best,\n",
      "inghanl uth so come is to bre ignes and ruponsh\n",
      "Pre this as ands to condaty;\n",
      "What blave, shall ting lad so on and be mindiners?\n",
      "Lown our My lord; had thinghares\n",
      "What nay, more you?\n",
      "We senlll sa this iftord;\n",
      "And thu, now, the sty a facing thoon\n",
      "t thue wartt hat and sofor trarl Obe\n",
      "To trot carsugm renor don wreicel of\n",
      "Bulod gid vill pren frorsal a the be daze,\n",
      "It of lad Gerst; a climbbneard.\n",
      "\n",
      "\n",
      "BARIND:\n",
      "Dum gret?\n",
      "I:\n",
      "Rize, ust go yefet my floud ingrivet wernt ofe\n",
      "She mae fach tome down he well war gloownt\n",
      "And tray, gitonag,', e werifful where bay to\n",
      "cont it flet'les of Gesse dought lat shine ning arqeid,\n",
      "where it dad upon And you do wit!\n",
      "QUELAn him fold; 'The to-here learndston fr hingbl,\n",
      "Four leavps tonle evart and conf. G-whitl.\n",
      "\n",
      "Set Dikelled spon the didelst Monceady onf berstlbe.\n",
      "\n",
      "\n",
      "MEDSI:\n",
      "ring fole.\n",
      "I You in in now to wity, dixtres, Aubong wegnt.\n",
      "\n",
      "Firse:\n",
      "But de do hat le staporty, mler, than, a son cenan bind\n",
      "Farets trurgh: of be I ise hou grt\n",
      "the ret give geturbup breignis.\n",
      "Not it antring to\n",
      "Anarce in\n",
      "Comg on Boor hongous? Cof hikeer thee fier palate what dixght.\n",
      "\n",
      "TLOMEDIUS:\n",
      "\n",
      "Goure stake bucir with all wincant your wowng?\n",
      "\n",
      "VIros'lll:\n",
      "Inow weppy thoughtht his dobe vianke.\n",
      "Wheill for I girse monoce, whith uto this neare dwo\n",
      "Het are gave haves hert's loved e dowfon.\n",
      "\n",
      "Dow Marderfo, had will tie ae to she I cond\n",
      "Ath like hersold more creds I uppak liest: wilf me\n",
      "Ist itheer coive is deatht! sepe mverert.\n",
      "It wargong berice, if his as flatin toumrge.\n",
      "WAll sar thin now th is not not nimrge:\n",
      "Burss.\n",
      "YLifece that,\n",
      "\n",
      "LORe nad, lord I will pasin no himens our light chis and peatl?\n",
      "\n",
      "ADY Jucrst eretindis yor furd ckikim heabldes\n",
      "Tuser and this his chen triene: Ang or,\n",
      "Mas, camm goronning the duke opr trik; 'Amon,\n",
      "nes a it.\n",
      "\n",
      "CAPULO:\n",
      "O, soule neave to,--nee ond has bater's shoul alt strongg to\n",
      "Alll in gshe deyeve, thats a de gra ublde's.\n",
      "\n",
      "TORGROK:\n",
      "I hexppit,--an forrd one it his\n",
      "Beceat'sy a upon grarn tokerss blave his\n",
      "Thit profonn as terpak: windany\n",
      "I madeing ofor and-fan-but on, not me confure\n",
      "Shall fladiss the for spetts?\n",
      "For Myrel out it speadiv of und move.\n",
      "Has hes here spat all sing bes to himse.\n",
      "Wihy bome don that, I, losant mome,\n",
      "You comvanving sescar Jiathines, you spors him\n",
      "'ll it deved in therse han gaued crefort dought oly\n",
      "Wetsetil; natt tound the willf ofer of sbuear wecets.\n",
      "\n",
      "ADWith, mistle firty havellf; From umpee head dee.\n",
      "But love fad, a his ares depadiness thee Tas\n",
      "Call, Inot a sewo gituearnd.\n",
      "\n",
      "\n",
      "ANCHE:\n",
      "What havitsts\n",
      "Sare the do fome soo aswiting I\n",
      "Homandalys becst is mitile here you, ofal with madst\n",
      "And garll: af Frizerst' yor which the ober'ss\n",
      "A in dit your ben dalshene shing funt,\n",
      "Sbeary see be te thus grom: wit?\n",
      "Us thise my pandens'ive dave to feed atchiun\n",
      "There notw't icrtors, whe king; now I sto gur a wo.\n",
      "\n",
      "BASHARGARE:\n",
      "With is thee this I enpoceeat; notlKourn blert a\n",
      "Way, shold toort a blod as ingsterper the\n",
      "Plaved, Sicths and thave to ofr carter to\n",
      "Your a avin I that Godare thught nie.\n",
      "\n",
      "NORIT:\n",
      "Apladis be ble unges, ir troo prizact,\n",
      "The hoy wito nerve.\n",
      "\n",
      "JULY EONTUS:\n",
      "Anday, I's dingrm, and thy nough ras!\n",
      "We him de not bon the pastwer'sting feest, evitt.\n",
      "\n",
      "LERDIIUS:\n",
      "Sayn:\n",
      "NO Mrim me, that dought all but his a helven anceve;\n",
      "Is dot surparss hinbouse ally my cunge;\n",
      "ho thy it cond a mase lifh shing.\n",
      "I ce the but dort, fore this do fentll and;\n",
      "Soo in tame casil not fordiownt thing\n",
      "I'lll he? lady do with I do:\n",
      "Than incenffy in and a cetting you, cand,\n",
      "Nou ware us eardle, king a myoun and wich bubliods\n",
      "Theey loovickes there frownd me id stimedsstsss\n",
      "My there now hemmelugcke, I ell ak, knowences.\n",
      "You, my presor, do tspeadiry eave!\n",
      "Son mating trot the my themby a alls a gels\n",
      "To sbestet witll.\n",
      "\n",
      "ORThe naw shipwith dogbled\n",
      "A hump good I sue paco sur soon house viling e'esp\n",
      "Shich shy man may goong, you shought.\n",
      "\n",
      "\n",
      "HEDWARY:\n",
      "Con I mir dam of munis. Bee you himsly:\n",
      "Whas dimir s garest we alless; reallt.\n",
      "Well with with qued inhor Sourd ead truk\n",
      "Thas shalll. But this witichy\n",
      "Thell dead, Garres. Shect healt' my ganw dive\n",
      "If best isly. Marest apwen.\n",
      "\n",
      "GESTERSTHAS:\n",
      "Wort heas exitler, reght whall me perdencess dut\n",
      "Fors wo. Buldoot, theinke thelvel.\n",
      "\n",
      "\n",
      "SHESBRUS:\n",
      "Shouse wite; all bind wennts.\n",
      "\n",
      "\n",
      "RICIO:\n",
      "Swee your set in it tha prarck rupave wuptencears\n",
      "And:\n",
      "Foorck schace fifet on.\n",
      "APlearen lorge have at blawter to wall:\n",
      "Hand ende, peaccork, Jory biniticy-n't\n",
      "Meprtigh it colnged wis boourdered,\n",
      "Them sroves com by cand thou nor I you.\n",
      "Whiple'ds cort: byo this you deater, youn!\n",
      "To the woulld is you, throu our argred beaintt hats\n",
      "mid in your Dovet thring this how\n",
      "Who nobare to grese had was in for; whath howarst\n",
      "All ale to: Furion; the go?\n",
      "Whater our sin cedomsering. I be come:\n",
      "A a sof wech; your argingbter Mage\n",
      "Soard'd the chis is to mare of you, inty\n",
      "Tothell\n",
      "Would thone comer, chy with is. hus go allis gre wit.\n",
      "A lord, the at as a mecined to you, thouge\n",
      "Sing plok on my, you, Cryof deannd me,,\n",
      "Soni we rep donot the much he wigh spom' tha'd too\n",
      "thim the nown the not freal, shour dinag on\n",
      "My irst sey ling to ther:\n",
      "WIver stish condiely boant pennar, dikes and\n",
      "Wom omoderss.\n",
      "\n",
      "RIS YORt RIVARLE: dusare lal a goood e do.\n",
      "Which's this shined thus will the hereas.\n",
      "\n",
      "LEONES:\n",
      "I hart than grive; is grres, Soressiend, whun enty,\n",
      "Sites thrings. Fairfess that shears's he frore nof\n",
      "Which drexes thall's there Ton lavicelf.\n",
      "\n",
      "\n",
      "KE VINTIOLA:\n",
      "Onot hast wildand shis dis, I to gumpakinjy st lown\n",
      "Where shings pat einswer, in whict rip.\n",
      "\n",
      "'That and the morn:\n",
      "I mard lake shart anace trurbycion will\n",
      "It of jigl ordg the Jeth.\n",
      "\n",
      "BENTER:\n",
      "ADs, twe no: you; not gritter:\n",
      "Where leelser of as premacife.\n",
      "\n",
      "CLISABET:\n",
      "Maronsme cherie on with rech in brat,\n",
      "Marenjom'dy.\n",
      "\n",
      "Nor-look for that, you fate thou plet frightheld with hat\n",
      "To machy thou hous ont cared:\n",
      "Son I'll ing! my ids aun it fon gacorighta oul?\n",
      "\n",
      "Thy swilld you fimatees don upputon ealls your batht\n",
      "ind wweris?\n",
      "\n",
      "ASINGSA:\n",
      "O whin resosty hermar, lok andliss, and is of lorng\n",
      "Of carcy woblal, diset!\n",
      "\n",
      "Somet whilll thinks wirng kelll is, Many\n",
      "Fr EOR VI:\n",
      "And seryep at the hof thim ment mese in to-\n",
      "you on tcond, in meat forcor ast I\n",
      "But mal chaversen my yound you cinth hear son tame\n",
      "The comysent laind thou lightes sa, bare lown--\n",
      "Which hou to the whith chill presone, gufresets\n",
      "Brood trone besices shall her hat, if toree ton terver?\n",
      "\n",
      "But toe.\n",
      "A Cautionses bent a wold should his\n",
      "Uptn ped onot this Mailly. Wit Pantor's my ors\n",
      "My Sis sit forve too by dunst sore,\n",
      "Marde sould would got windy wern 'nat,\n",
      "Ter gooond evind serced reave who no's the drother your\n",
      "Bich and.\n",
      "\n",
      "\n",
      "RICHORGLANGANCE:\n",
      "Dearst bunt of Eneary wis not, of a.\n",
      "\n",
      "CORIS EOLBRT:\n",
      "O, low is bepare,\n",
      "And a and woulld to thatcch a forch me:\n",
      "Atme name thy whith sulice of but whard of les thim ters\n",
      "I thougixe, earse yet lord; won buld, lukes tit\n",
      "Thoped insabi harsed's fivend is hines fains n midst.\n",
      "Harw on Mord ggrieat, ther shild thee's,\n",
      "What some of this herdie palive you wror ive oure\n",
      "Your we pore lifuld ad\n",
      "Spobly goiexe of Ence ing you?\n",
      "\n",
      "Nould cwopl may him llixe\n",
      "cor Ohisulle on arealing prave tands fliol?\n",
      "\n",
      "ORows you canavandioess, he and tart touge. Has\n",
      "To and done to arn; I oneap.\n",
      "\n",
      "Nor ESpalmn, you her fecenco,\n",
      "Whack as as car dake risme?\n",
      "\n",
      "\n",
      "RIOLIGBZANGES:\n",
      "Com, fleessie of the vome lower and,\n",
      "Nown, the sing! Whinds thy\n",
      "\n",
      "Dide loven a an This himer?\n",
      "When frive a beges ext swet uleds.\n",
      "\n",
      "AULTHARD II fall: and wade\n",
      "On nor plovent falter isspase the me bed,\n",
      "think in tire; and Rillowldss; takin?\n",
      "Is you mowt; I prhitingms on culit.\n",
      "\n",
      "\n",
      "THESBRIES:\n",
      "Mon it tharrled:\n",
      "He his that con were cantard tough ther of-\n",
      "But croveld; jorse lier you col.\n",
      "\n",
      "HEFLORY:\n",
      "Breces min to are oxt, non's me dithty writ bom.\n",
      "\n",
      "DUKINGA:\n",
      "And no thanbe'shing crurk for be a is En uthat umpplent?\n",
      "\n",
      "Plive chapun swilt my your a if dithen,\n",
      "And of quen fordary.; Be's in I the no\n",
      "Und's tharre eabh patingan whatt's tremer hand,\n",
      "Upin ale not lord?\n",
      "\n",
      "\n",
      "FRIJBED:\n",
      "Forilan Vactes know: sue on by Misind.\n",
      "\n",
      "\n",
      "the my sir garde, thoush, and my lord, riast juiind thear oae cant\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx, max_new_tokens=10000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e667b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
